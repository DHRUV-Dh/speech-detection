{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f86b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import librosa\n",
    "import os\n",
    "# Set the path to the directory containing the audio files\n",
    "audio_dir = \"/Users/apple/Desktop/speech/Audio_Speech_Actors_01-24/Actor_01\"  # Update with the path to your audio files directory\n",
    "\n",
    "# Set the maximum length for padding/truncating the audio features\n",
    "max_length = 500  # Update with your desired maximum length\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (e.g., Mel-frequency cepstral coefficients - MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Flatten the MFCCs to obtain a feature vector\n",
    "        audio_features = mfccs.flatten()\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(audio_features)\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        label = filename.split(\"_\")[0]  # Assumes the file name follows a specific format\n",
    "        labels.append(label)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Scale the training features\n",
    "X_test_scaled = scaler.transform(X_test)  # Scale the testing features\n",
    "\n",
    "# Train the machine learning model\n",
    "model = SVC(kernel='linear')  # Choose an appropriate algorithm\n",
    "model.fit(X_train_scaled, y_train)  # Train the model\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = model.score(X_train_scaled, y_train)  # Evaluate training accuracy\n",
    "test_accuracy = model.score(X_test_scaled, y_test)  # Evaluate testing accuracy\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Save the trained model\n",
    "# Assuming you want to save the model as a file 'emotion_detection_model.pkl'\n",
    "# import pickle\n",
    "# with open(\"emotion_detection_model.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5b90463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.8020833333333334\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the path to the directory containing the audio files\n",
    "audio_dir = \"/Volumes/Time Machine Backups/Desktop/speech/Audio_Speech_Actors_01-24/all\"\n",
    "\n",
    "# Define the maximum length for padding/truncating the audio features\n",
    "max_length = 500\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Emotions in the dataset\n",
    "emotions = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "observed_emotions = [\"calm\", \"happy\", \"fearful\", \"disgust\"]\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=1024)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Flatten the MFCCs to obtain a feature vector\n",
    "        audio_features = mfccs.flatten()\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        parts = filename.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping file with invalid name: {filename}\")\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], \"unknown\")\n",
    "        \n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(audio_features)\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=9)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the machine learning model (SVM)\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = model.score(X_train_scaled, y_train)\n",
    "test_accuracy = model.score(X_test_scaled, y_test)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# # Save the trained model if needed\n",
    "# import pickle\n",
    "# with open(\"emotion_detection_model_new_4_emo.pkl\", \"wb\") as file:\n",
    "#    pickle.dump(model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b07e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.6194444444444445\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the path to the directory containing the audio files\n",
    "audio_dir = \"/Volumes/Time Machine Backups/Desktop/speech/Audio_Speech_Actors_01-24/all\"\n",
    "\n",
    "# Define the maximum length for padding/truncating the audio features\n",
    "max_length = 500\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Emotions in the dataset\n",
    "emotions = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "observed_emotions = list(emotions.values())  # Use all emotions from the dictionary\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=1024)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Flatten the MFCCs to obtain a feature vector\n",
    "        audio_features = mfccs.flatten()\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        parts = filename.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping file with invalid name: {filename}\")\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], \"unknown\")\n",
    "        \n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(audio_features)\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=9)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the machine learning model (SVM)\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = model.score(X_train_scaled, y_train)\n",
    "test_accuracy = model.score(X_test_scaled, y_test)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# # Save the trained model if needed\n",
    "import pickle\n",
    "with open(\"emotion_detection_model_new_all_emo.pkl\", \"wb\") as file:\n",
    "     pickle.dump(model, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d71db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bf1705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.7083333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.75      0.85      0.80        54\n",
      "     disgust       0.70      0.70      0.70        44\n",
      "     fearful       0.76      0.60      0.67        53\n",
      "       happy       0.60      0.66      0.63        41\n",
      "\n",
      "    accuracy                           0.71       192\n",
      "   macro avg       0.71      0.70      0.70       192\n",
      "weighted avg       0.71      0.71      0.71       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the path to the directory containing the audio files\n",
    "audio_dir = \"/Volumes/Time Machine Backups/Desktop/speech/Audio_Speech_Actors_01-24/all\"\n",
    "\n",
    "# Define the maximum length for padding/truncating the audio features\n",
    "max_length = 500\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Emotions in the dataset\n",
    "emotions = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "observed_emotions = [\"calm\", \"happy\", \"fearful\", \"disgust\"]\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=1024)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Flatten the MFCCs to obtain a feature vector\n",
    "        audio_features = mfccs.flatten()\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        parts = filename.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping file with invalid name: {filename}\")\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], \"unknown\")\n",
    "        \n",
    "        if emotion not in observed_emotions:\n",
    "            continue\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(audio_features)\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=9)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "train_accuracy = best_model.score(X_train_scaled, y_train)\n",
    "test_accuracy = best_model.score(X_test_scaled, y_test)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predictions and classification report\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afa8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"emotion_detection_model_new_4.70_emo.pkl\", \"wb\") as file:\n",
    "     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5ad3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n",
      "Testing Accuracy: 0.49444444444444446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.69      0.55      0.61        53\n",
      "        calm       0.54      0.75      0.63        53\n",
      "     disgust       0.52      0.58      0.55        52\n",
      "     fearful       0.49      0.45      0.47        42\n",
      "       happy       0.42      0.27      0.33        51\n",
      "     neutral       0.29      0.26      0.28        19\n",
      "         sad       0.26      0.20      0.23        44\n",
      "   surprised       0.52      0.70      0.59        46\n",
      "\n",
      "    accuracy                           0.49       360\n",
      "   macro avg       0.47      0.47      0.46       360\n",
      "weighted avg       0.49      0.49      0.48       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the path to the directory containing the audio files\n",
    "audio_dir = \"/Volumes/Time Machine Backups/Desktop/speech/Audio_Speech_Actors_01-24/all\"\n",
    "\n",
    "# Define the maximum length for padding/truncating the audio features\n",
    "max_length = 500\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Emotions in the dataset\n",
    "emotions = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=1024)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Flatten the MFCCs to obtain a feature vector\n",
    "        audio_features = mfccs.flatten()\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        parts = filename.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping file with invalid name: {filename}\")\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], \"unknown\")\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(audio_features)\n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=9)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best model from grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model\n",
    "train_accuracy = best_model.score(X_train_scaled, y_train)\n",
    "test_accuracy = best_model.score(X_test_scaled, y_test)\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Predictions and classification report\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b0bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"emotion_detection_model_new_all.49_emo.pkl\", \"wb\") as file:\n",
    "     pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8543dade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "33/33 [==============================] - 182s 5s/step - loss: 39.2997 - accuracy: 0.1306 - val_loss: 2.0456 - val_accuracy: 0.1306\n",
      "Epoch 2/50\n",
      "33/33 [==============================] - 2787s 85s/step - loss: 2.0717 - accuracy: 0.1509 - val_loss: 2.0471 - val_accuracy: 0.1611\n",
      "Epoch 3/50\n",
      "33/33 [==============================] - 9009s 275s/step - loss: 2.1909 - accuracy: 0.1546 - val_loss: 2.0583 - val_accuracy: 0.1472\n",
      "Epoch 4/50\n",
      "33/33 [==============================] - 272s 8s/step - loss: 2.0554 - accuracy: 0.1546 - val_loss: 2.0365 - val_accuracy: 0.1694\n",
      "Epoch 5/50\n",
      "33/33 [==============================] - 170s 5s/step - loss: 2.0529 - accuracy: 0.1685 - val_loss: 2.0074 - val_accuracy: 0.2083\n",
      "Epoch 6/50\n",
      "33/33 [==============================] - 198s 6s/step - loss: 2.0454 - accuracy: 0.1843 - val_loss: 2.0097 - val_accuracy: 0.2028\n",
      "Epoch 7/50\n",
      "33/33 [==============================] - 256s 8s/step - loss: 2.0409 - accuracy: 0.1759 - val_loss: 2.0002 - val_accuracy: 0.2500\n",
      "Epoch 8/50\n",
      "33/33 [==============================] - 286s 8s/step - loss: 2.0508 - accuracy: 0.1713 - val_loss: 2.0196 - val_accuracy: 0.1944\n",
      "Epoch 9/50\n",
      "33/33 [==============================] - 265s 8s/step - loss: 2.0354 - accuracy: 0.1611 - val_loss: 2.0019 - val_accuracy: 0.2000\n",
      "Epoch 10/50\n",
      "33/33 [==============================] - 240s 7s/step - loss: 2.0282 - accuracy: 0.1843 - val_loss: 1.9909 - val_accuracy: 0.2194\n",
      "Epoch 11/50\n",
      "33/33 [==============================] - 245s 7s/step - loss: 2.0334 - accuracy: 0.1620 - val_loss: 2.0009 - val_accuracy: 0.1861\n",
      "Epoch 12/50\n",
      "33/33 [==============================] - 301s 9s/step - loss: 2.0401 - accuracy: 0.1593 - val_loss: 1.9952 - val_accuracy: 0.1806\n",
      "Epoch 13/50\n",
      "33/33 [==============================] - 219s 6s/step - loss: 2.0333 - accuracy: 0.1731 - val_loss: 1.9917 - val_accuracy: 0.2111\n",
      "Epoch 14/50\n",
      "33/33 [==============================] - 207s 6s/step - loss: 2.0191 - accuracy: 0.1981 - val_loss: 1.9720 - val_accuracy: 0.1972\n",
      "Epoch 15/50\n",
      "33/33 [==============================] - 258s 8s/step - loss: 2.0302 - accuracy: 0.1639 - val_loss: 1.9767 - val_accuracy: 0.2111\n",
      "Epoch 16/50\n",
      "33/33 [==============================] - 210s 6s/step - loss: 2.0252 - accuracy: 0.1954 - val_loss: 1.9804 - val_accuracy: 0.2139\n",
      "Epoch 17/50\n",
      "33/33 [==============================] - 327s 10s/step - loss: 2.0313 - accuracy: 0.1806 - val_loss: 1.9826 - val_accuracy: 0.1944\n",
      "Epoch 18/50\n",
      "33/33 [==============================] - 349s 10s/step - loss: 2.0148 - accuracy: 0.1935 - val_loss: 1.9724 - val_accuracy: 0.2139\n",
      "Epoch 19/50\n",
      "33/33 [==============================] - 251s 7s/step - loss: 2.0566 - accuracy: 0.2056 - val_loss: 1.9581 - val_accuracy: 0.2361\n",
      "Epoch 20/50\n",
      "33/33 [==============================] - 1861s 57s/step - loss: 2.0189 - accuracy: 0.1944 - val_loss: 1.9673 - val_accuracy: 0.2111\n",
      "Epoch 21/50\n",
      "33/33 [==============================] - 164s 5s/step - loss: 2.0310 - accuracy: 0.1657 - val_loss: 1.9874 - val_accuracy: 0.2111\n",
      "Epoch 22/50\n",
      "33/33 [==============================] - 149s 4s/step - loss: 2.1345 - accuracy: 0.1917 - val_loss: 1.9951 - val_accuracy: 0.1833\n",
      "Epoch 23/50\n",
      "33/33 [==============================] - 143s 4s/step - loss: 2.0280 - accuracy: 0.1935 - val_loss: 1.9953 - val_accuracy: 0.2056\n",
      "Epoch 24/50\n",
      "33/33 [==============================] - 134s 4s/step - loss: 1.9991 - accuracy: 0.2065 - val_loss: 1.9982 - val_accuracy: 0.2194\n",
      "12/12 [==============================] - 12s 999ms/step - loss: 1.9982 - accuracy: 0.2194\n",
      "Testing Loss: 1.9982143640518188\n",
      "Testing Accuracy: 0.21944443881511688\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define the path to the directory containing the audio files\n",
    "audio_dir = \"/Volumes/Time Machine Backups/Desktop/speech/Audio_Speech_Actors_01-24/all\"\n",
    "\n",
    "# Define the maximum length for padding/truncating the audio features\n",
    "max_length = 500\n",
    "\n",
    "# Create empty lists to store the features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Emotions in the dataset\n",
    "emotions = {\n",
    "    \"01\": \"neutral\",\n",
    "    \"02\": \"calm\",\n",
    "    \"03\": \"happy\",\n",
    "    \"04\": \"sad\",\n",
    "    \"05\": \"angry\",\n",
    "    \"06\": \"fearful\",\n",
    "    \"07\": \"disgust\",\n",
    "    \"08\": \"surprised\"\n",
    "}\n",
    "\n",
    "# Loop through each audio file in the directory\n",
    "for filename in os.listdir(audio_dir):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        # Load the audio file\n",
    "        file_path = os.path.join(audio_dir, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Extract audio features (MFCCs)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40, n_fft=1024)\n",
    "        \n",
    "        # Pad or truncate the audio features to the maximum length\n",
    "        if mfccs.shape[1] < max_length:\n",
    "            pad_width = max_length - mfccs.shape[1]\n",
    "            mfccs = np.pad(mfccs, ((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_length]\n",
    "        \n",
    "        # Append the features and labels to the respective lists\n",
    "        features.append(mfccs)\n",
    "        \n",
    "        # Extract the emotion label from the file name\n",
    "        parts = filename.split(\"-\")\n",
    "        if len(parts) < 3:\n",
    "            print(f\"Skipping file with invalid name: {filename}\")\n",
    "            continue\n",
    "        emotion = emotions.get(parts[2], \"unknown\")\n",
    "        \n",
    "        labels.append(emotion)\n",
    "\n",
    "# Convert the lists to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Convert emotion labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.25, random_state=9)\n",
    "\n",
    "# Reshape features for CNN input\n",
    "X_train = X_train[..., np.newaxis]  # Add channel dimension\n",
    "X_test = X_test[..., np.newaxis]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    shear_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Initialize the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(40, max_length, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(datagen.flow(X_train, y_train_onehot, batch_size=32),\n",
    "                    steps_per_epoch=len(X_train) / 32, epochs=50,\n",
    "                    validation_data=(X_test, y_test_onehot),\n",
    "                    callbacks=[callbacks.EarlyStopping(patience=5)])\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_onehot)\n",
    "print(\"Testing Loss:\", test_loss)\n",
    "print(\"Testing Accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
